{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ThRPaYIRNDv8",
      "metadata": {
        "id": "ThRPaYIRNDv8"
      },
      "source": [
        "# A Colab notebook for PhosBoost phosphorylation prediction\n",
        "\n",
        "PhosBoost is a machine learning approach that leverages protein language models and gradient boosting trees (CatBoost, specifically) to predict protein phosphorylation from experimentally derived data. PhosBoost offers improved performance when recall is prioritized while consistently providing more confident probability scores. You can find more information in the [Plant Direct manuscript](https://onlinelibrary.wiley.com/doi/10.1002/pld3.554) and in the [GitHub repository](https://github.com/eporetsky/PhosBoost).\n",
        "\n",
        "This Colab notebook was written to allow predict phosphorylated residues in a given protein sequence. It is designed to run a CPU node and takes less then a minute to setup and run the prediction. Once everything is complete you will be prompted to download the CSV file with the phosphorylation prediction results for the S/T/Y residues. The output includes the predicted probabilities for each of the residues and the phosphorylation prediction (1 if probability > 0.5, 0 otherwise).\n",
        "\n",
        "**NOTE:** PhosBoost was trained on the experimental plant phosphorylation data using the QPTMPlants database and has not been tested on non-plant species. See the GitHub for instructions on training your own model or either send me an email or create an GitHub issue and I will try to help!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd6f0c8f-d9db-4623-a2ee-483008dc9d74",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd6f0c8f-d9db-4623-a2ee-483008dc9d74",
        "outputId": "217a2aaa-aa33-47e0-ad5e-3f6a668d2b81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu\n",
            "2024-05-01 03:52:19 URL:https://raw.githubusercontent.com/eporetsky/PhosBoost/main/data/models/qPTMplants.ST.joblib.pkl [8052716/8052716] -> \"qPTMplants.ST.joblib.pkl\" [1]\n",
            "2024-05-01 03:52:20 URL:https://raw.githubusercontent.com/eporetsky/PhosBoost/main/data/models/qPTMplants.Y.joblib.pkl [3736762/3736762] -> \"qPTMplants.Y.joblib.pkl\" [1]\n"
          ]
        }
      ],
      "source": [
        "#@title Setup the environment to run PhosBoost (Run this cell once at the beginning)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "!uv pip install torch torchvision torchaudio transformers sentencepiece accelerate h5py biopython catboost==1.2.1 --extra-index-url https://download.pytorch.org/whl/cu116 > /dev/null\n",
        "\n",
        "# Import dependencies\n",
        "from transformers import T5EncoderModel, T5Tokenizer\n",
        "import torch\n",
        "import h5py\n",
        "import time\n",
        "from Bio import SeqIO\n",
        "\n",
        "import re\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Path variables\n",
        "per_protein_path = \"./protT5/output/per_protein_embeddings.h5\" # where to store the embeddings\n",
        "\n",
        "# check whether GPU is available\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using {device}\")\n",
        "\n",
        "# Load ProtT5 in half-precision (more specifically: the encoder-part of ProtT5-XL-U50)\n",
        "def get_T5_model():\n",
        "    '''\n",
        "    retrieves the model and tokenizer\n",
        "    '''\n",
        "    # specify the encorder-part of the model\n",
        "    model_checkpoint = 'Rostlab/prot_t5_xl_half_uniref50-enc'\n",
        "\n",
        "    # import the model\n",
        "    model = T5EncoderModel.from_pretrained(model_checkpoint)\n",
        "\n",
        "    model = model.to(device) # move model to GPU\n",
        "    model = model.eval() # set model to evaluation model\n",
        "\n",
        "    # import tokenizer\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# loading model\n",
        "model, tokenizer = get_T5_model()\n",
        "\n",
        "# Download the PhosBoost pickled CatBoost models to run the classification\n",
        "!wget --no-verbose -O qPTMplants.ST.joblib.pkl https://github.com/eporetsky/PhosBoost/blob/main/data/models/qPTMplants.ST.joblib.pkl?raw=true\n",
        "!wget --no-verbose -O qPTMplants.Y.joblib.pkl https://github.com/eporetsky/PhosBoost/blob/main/data/models/qPTMplants.Y.joblib.pkl?raw=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bSP7y608aKgI",
      "metadata": {
        "cellView": "form",
        "id": "bSP7y608aKgI"
      },
      "outputs": [],
      "source": [
        "#@title Change the sequence and ID and hit `Runtime` -> `Run after`\n",
        "\n",
        "seq_id = \"AT4G18710\" #@param {type:\"string\"}\n",
        "seq = \"MADDKEMPAAVVDGHDQVTGHIISTTIGGKNGEPKQTISYMAERVVGTGSFGIVFQAKCLETGETVAIKKVLQDRRYKNRELQLMRVMDHPNVVCLKHCFFSTTSKDELFLNLVMEYVPESLYRVLKHYSSANQRMPLVYVKLYMYQIFRGLAYIHNVAGVCHRDLKPQNLLVDPLTHQVKICDFGSAKQLVKGEANISYICSRFYRAPELIFGATEYTTSIDIWSAGCVLAELLLGQPLFPGENAVDQLVEIIKVLGTPTREEIRCMNPHYTDFRFPQIKAHPWHKIFHKRMPPEAIDFASRLLQYSPSLRCTALEACAHPFFDELREPNARLPNGRPFPPLFNFKQEVAGSSPELVNKLIPDHIKRQLGLSFLNQSGT*\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "261698df-2497-4f6d-89ef-1df09736110a",
      "metadata": {
        "cellView": "form",
        "id": "261698df-2497-4f6d-89ef-1df09736110a"
      },
      "outputs": [],
      "source": [
        "#@title This cell calculates the PLM embeddings for the S/T/Y residues\n",
        "\n",
        "def get_ixs_sites(name, seq, sites):\n",
        "    \"\"\"\n",
        "    Returns\n",
        "    \"\"\"\n",
        "    ix_list = []\n",
        "    site_list = []\n",
        "    for pattern in sites:\n",
        "        indices = re.finditer(pattern=pattern, string=seq)\n",
        "        for ix in indices:\n",
        "            ix = ix.start()\n",
        "            ix_list.append(ix)\n",
        "            site_list.append(name+\"_\"+pattern+str(ix+1))\n",
        "\n",
        "    return(site_list, ix_list)\n",
        "\n",
        "def get_embeddings(model, tokenizer, seq, seq_id,\n",
        "                   max_residues=50000, max_seq_len=20000, max_batch=1000 ):\n",
        "# original: max_residues=4000, max_seq_len=1000, max_batch=100\n",
        "    seq.replace(\"*\", \"\").replace(\"\\n\", \"\").replace(\" \", \"\")\n",
        "    seq_len = len(seq)\n",
        "    seqs = (' '.join(list(seq)), )\n",
        "\n",
        "    token_encoding = tokenizer.batch_encode_plus(seqs, add_special_tokens=True, padding=\"longest\")\n",
        "    input_ids = torch.tensor(token_encoding['input_ids']).to(device)\n",
        "    attention_mask = torch.tensor(token_encoding['attention_mask']).to(device)\n",
        "\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            # returns: ( batch-size x max_seq_len_in_minibatch x embedding_dim )\n",
        "            embedding_repr = model(input_ids, attention_mask=attention_mask)\n",
        "    except RuntimeError:\n",
        "        print(\"RuntimeError during embedding for {} (L={})\".format(seq_id, seq_len))\n",
        "\n",
        "    # Get the site names and the ixs in the numpy array\n",
        "    sites, ixs = get_ixs_sites(seq_id, seq, \"STY\")\n",
        "\n",
        "    emb = embedding_repr.last_hidden_state[0,:seq_len]\n",
        "\n",
        "    tmp_emb = np.hstack([\n",
        "        emb.detach().cpu().numpy().squeeze()[ixs,:],\n",
        "        np.tile(emb.mean(dim=0).detach().cpu().numpy().squeeze(), (len(ixs), 1))\n",
        "    ])\n",
        "\n",
        "    # Add the generated ProtT5 embeddings\n",
        "    results = np.empty((0, 2048))\n",
        "    results = np.vstack([results, tmp_emb])\n",
        "    results = pd.DataFrame(results)\n",
        "    results.index = sites\n",
        "    return results\n",
        "\n",
        "results = get_embeddings(model, tokenizer, seq, seq_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fhGjaOi6hU6e",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fhGjaOi6hU6e",
        "outputId": "cdcef19d-36bb-4055-e21a-4448f6dad6dd"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_e6caced4-caef-4b19-b6f6-4e3c3f40ef32\", \"AT4G18710.phosboost.csv\", 1908)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PhosBoost prediction was completed for AT4G18710\n"
          ]
        }
      ],
      "source": [
        "#@title This cells performs the phosphorylation prediction using the PhosBoost method\n",
        "\n",
        "preds_df = pd.DataFrame()\n",
        "\n",
        "# Name of the embedding file(s) located in the data/embeddings/ folder for model training\n",
        "# Optional: To combine multiple embedding files comma-separate each input file name\n",
        "# (For example: combining training and validation data after hyperparameter tuning)\n",
        "# Load the embedding data file(s) for the model to be trained on\n",
        "X = results.copy()\n",
        "res_list = X.index.tolist()\n",
        "X[\"res\"] = X.index.str.split(\"_\").str[-1].str[0]\n",
        "X = X[X[\"res\"].isin([\"S\", \"T\"])]\n",
        "X = X.drop(\"res\", axis=1)\n",
        "cb_model = joblib.load(\"qPTMplants.ST.joblib.pkl\")\n",
        "preds_proba = cb_model.predict_proba(X)\n",
        "preds_ST = pd.DataFrame()\n",
        "preds_ST[\"site\"] = X.index.tolist()\n",
        "preds_ST[\"probability\"]  = preds_proba[:,1]\n",
        "preds_ST[\"prediction\"]  = preds_ST[\"probability\"].apply(lambda x: 1 if x >= 0.5 else 0)\n",
        "\n",
        "X = results.copy()\n",
        "res_list = X.index.tolist()\n",
        "X[\"res\"] = X.index.str.split(\"_\").str[-1].str[0]\n",
        "X = X[X[\"res\"].isin([\"Y\"])]\n",
        "X = X.drop(\"res\", axis=1)\n",
        "cb_model = joblib.load(\"qPTMplants.Y.joblib.pkl\")\n",
        "preds_proba = cb_model.predict_proba(X)\n",
        "preds_Y = pd.DataFrame()\n",
        "preds_Y[\"site\"] = X.index.tolist()\n",
        "preds_Y[\"probability\"]  = preds_proba[:,1]\n",
        "preds_Y[\"prediction\"]  = preds_Y[\"probability\"].apply(lambda x: 1 if x >= 0.5 else 0)\n",
        "\n",
        "# Create the dataframe and download it\n",
        "preds_df = pd.concat([preds_ST, preds_Y])\n",
        "preds_df.to_csv(\"{}.phosboost.csv\".format(seq_id), index=False)\n",
        "from google.colab import files\n",
        "files.download(\"{}.phosboost.csv\".format(seq_id))\n",
        "\n",
        "print(\"PhosBoost prediction was completed for {}\".format(seq_id))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
